{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marvi\\Anaconda3\\envs\\rl2019\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n",
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env_cartpole = gym.envs.make(\"CartPole-v0\") # Has two actions, See doc for mor info, ??env_cartpole.env\n",
    "env_car = gym.envs.make(\"MountainCar-v0\")   # Has three actions\n",
    "env_pen = gym.envs.make(\"Pendulum-v0\")      # Has continious action values like array([-1.2552279] or array([1.7774895] \n",
    "env_acrobot = gym.envs.make(\"Acrobot-v1\")   # Has Three actions, applying +1, 0 or -1 torque on the joint between #\n",
    "                                            # the two pendulum links.   See doc for more info, ??env_acrobot.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??env_acrobot.env\n",
    "#env_acrobot.action_space.sample()\n",
    "#env = env_cartpole\n",
    "env = env_car\n",
    "#env = env_acrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  \n",
    "#device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "name = env.unwrapped.spec.id\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test demo environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, device,num_input=4,num_hidden=128,num_output=2):\n",
    "        nn.Module.__init__(self)\n",
    "        self.device = device\n",
    "        self.l1 = nn.Linear(num_input, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, num_output)\n",
    "        self.input_dim = num_input\n",
    "        self.hidden_dim = num_hidden\n",
    "        self.output_dim = num_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.Tensor(x)\n",
    "            \n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        forward_pass = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.ReLU(),\n",
    "            self.l2\n",
    "        )\n",
    "        \n",
    "        return forward_pass(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon ($\\epsilon$) greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(it):\n",
    "    # YOUR CODE HERE\n",
    "    epsilon = 1 - (min(it,1000) * 0.00095) #After 1000 iterations epsilon should be 0.05\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_schedule(it,max_iter,initial_value,final_value):\n",
    "    #Following general formula of get epislon\n",
    "    parameter_value = initial_value - (min(it,max_iter) * ( (initial_value - final_value) / max_iter) )\n",
    "                            \n",
    "    return parameter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon):\n",
    "    # YOUR CODE HERE\n",
    "    with torch.no_grad():\n",
    "        Q_approx = model(state)\n",
    "        a = int(np.random.rand() * model.output_dim) if np.random.rand() < epsilon else torch.argmax(Q_approx).item()\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replays types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.name = \"uniform\"\n",
    "\n",
    "    def push(self, transition):\n",
    "        \n",
    "        if len(self.memory) == self.capacity:\n",
    "            self.memory = self.memory[1:]   \n",
    "        \n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prioritized Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_tree import SumSegmentTree, MinSegmentTree\n",
    "\n",
    "#Source OpenAI: https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, size, alpha):\n",
    "        \"\"\"Create Prioritized Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        alpha: float\n",
    "            how much prioritization is used\n",
    "            (0 - no prioritization, 1 - full prioritization)\n",
    "        See Also\n",
    "        --------\n",
    "        ReplayBuffer.__init__\n",
    "        \"\"\"\n",
    "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
    "        assert alpha >= 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "        self.name = \"prioritized\"\n",
    "\n",
    "    def add(self, *args, **kwargs):\n",
    "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
    "        idx = self._next_idx\n",
    "        super().add(*args, **kwargs)\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        p_total = self._it_sum.sum(0, len(self._storage) - 1)\n",
    "        every_range_len = p_total / batch_size\n",
    "        for i in range(batch_size):\n",
    "            mass = random.random() * every_range_len + i * every_range_len\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        compared to ReplayBuffer.sample\n",
    "        it also returns importance weights and idxes\n",
    "        of sampled experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - no corrections, 1 - full correction)\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        weights: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "            denoting importance weight of each sampled transition\n",
    "        idxes: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "            idexes in buffer of sampled experiences\n",
    "        \"\"\"\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = np.array(weights)\n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return tuple(list(encoded_sample) + [weights, idxes])\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\n",
    "        sets priority of transition at index idxes[i] in buffer\n",
    "        to priorities[i].\n",
    "        Parameters\n",
    "        ----------\n",
    "        idxes: [int]\n",
    "            List of idxes of sampled transitions\n",
    "        priorities: [float]\n",
    "            List of updated priorities corresponding to\n",
    "            transitions at the sampled idxes denoted by\n",
    "            variable `idxes`.\n",
    "        \"\"\"\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = priority ** self._alpha\n",
    "            self._it_min[idx] = priority ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)\n",
    "            \n",
    "def train_prioritized(model, memory, optimizer, batch_size, discount_factor,beta,prioritized_replay_eps=1e-6):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    #----------------------------#Adjusted of normal train--------------------------------------------------------\n",
    "    transitions = memory.sample(batch_size, beta=beta)\n",
    "    (state, action, reward, next_state, done, weights, batch_idxes) = transitions#zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float, device = model.device)\n",
    "    action = torch.tensor(action, dtype=torch.int64, device = model.device)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float, device = model.device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float, device = model.device)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    weights = torch.tensor(weights, dtype=torch.float, device = model.device)\n",
    "    #batch_idxes = torch.tensor(batch_idxes, dtype = torch.int64, device = model.device)\n",
    "    \n",
    "     #-------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "    \n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    mean_loss_value = loss.item() \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        td_errors = F.smooth_l1_loss(q_val, target,reduction=\"none\") #TD errors, \n",
    "    #td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
    "        if model.device == \"cpu\":\n",
    "            td_errors = td_errors.detach().numpy()\n",
    "        else:\n",
    "            td_errors = td_errors.cpu().detach().numpy()\n",
    "    new_priorities = np.abs(td_errors) + prioritized_replay_eps\n",
    "    #new_priorities = np.abs(loss_value) + prioritized_replay_eps\n",
    "    memory.update_priorities(batch_idxes, new_priorities)\n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return mean_loss_value  # Returns a Python scalar, and releases history (similar to .detach())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selective Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveReplayMemory:\n",
    "    \n",
    "    def __init__(self, fifo_capacity, episodic_memory_capacity):\n",
    "        self.fifo_capacity = fifo_capacity\n",
    "        self.episodic_capacity = episodic_memory_capacity\n",
    "        self.fifo_memory = []\n",
    "        self.episodic_memory = []\n",
    "        self.name = \"selective\"\n",
    "\n",
    "    def push(self, transition):\n",
    "        \n",
    "        if len(self.fifo_memory) == self.fifo_capacity:\n",
    "            self.fifo_memory = self.fifo_memory[1:]   \n",
    "        \n",
    "        self.fifo_memory.append(transition)\n",
    "    \n",
    "        #Distribution matching selection\n",
    "        rank_value = np.random.standard_normal(1)\n",
    "        self.episodic_memory.append( (transition,rank_value) )\n",
    "        \n",
    "        if len(self.episodic_memory) > self.episodic_capacity:\n",
    "            sorted_episodic = sorted(self.episodic_memory, key=operator.itemgetter(1))\n",
    "            self.episodic_memory = sorted_episodic[1:]    #Smallest value is index 0 to be removed\n",
    "        \n",
    "        #Since no dictionairy was/could be used, duplicate states can be present in the episodic memory\n",
    "        #Or else could use dictionairy of state to rank value and dict state to other details of transition.\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        #50% chance to sample from either buffer?\n",
    "        #samples = random.sample(self.memory,batch_size)\n",
    "        #samples = random.sample(self.episodic_memory.keys(),batch_size)\n",
    "        \n",
    "        #Or concatenate and sample\n",
    "        concat = self.fifo_memory + [i[0] for i in self.episodic_memory]\n",
    "        samples = random.sample(concat,batch_size)\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fifo_memory) #+ len(episodic_memory) #Do we count both memories as len? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hindsight Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Epsilon_Greedy_Goal(nn.Module):\n",
    "    \n",
    "    def __init__(self,device,num_input,num_hidden,num_output):\n",
    "        nn.Module.__init__(self)\n",
    "        self.device = device\n",
    "        self.l1 = nn.Linear(num_input, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, num_output)\n",
    "        self.input_dim = num_input  #2 times the state size\n",
    "        self.hidden_dim = num_hidden\n",
    "        self.output_dim = num_output\n",
    "        \n",
    "    def forward(self,model,state,epsilon,goal):\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if np.random.rand() < epsilon:\n",
    "                \n",
    "                combined = np.concatenate(state,goal)\n",
    "                if not torch.is_tensor(combined):\n",
    "                    combined_tensor = torch.Tensor(combined)\n",
    "\n",
    "                combined_tensor = combined_tensor.to(self.device)\n",
    "\n",
    "                forward_pass = nn.Sequential(\n",
    "                    self.l1,\n",
    "                    nn.ReLU(),\n",
    "                    self.l2\n",
    "                )\n",
    "                \n",
    "                output = forward_pass(combined_tensor)\n",
    "                a_probs = F.softmax(output)\n",
    "                \n",
    "                a = np.random.choice(len(a_probs), size=1,p=a_probs)\n",
    "            else:\n",
    "                Q_approx = model(state)\n",
    "                a = torch.argmax(Q_approx).item()\n",
    "        return a\n",
    "    \n",
    "class HindsightReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity, k,env_name):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.k = k\n",
    "        self.name = \"hindsight\"\n",
    "        self.goals = []\n",
    "        self.env_name = env_name\n",
    "        self.current_episode = []\n",
    "            \n",
    "        if env_name == \"Acrobot-v1\":\n",
    "            self.true_goal_func = lambda s: bool(-np.cos(s[0]) - np.cos(s[1] + s[0]) > 1.)\n",
    "            self.goal_func = lambda s1,s2: s1 == s2\n",
    "        elif env_name == \"MountainCar-v0\":\n",
    "            self.true_goal_func = lambda s: s[0] >= 0.5 \n",
    "            self.goal_func = lambda s1,s2: s1[0] == s2[0]\n",
    "        \n",
    "    def compute_reward(observed_state,goal):\n",
    "        \n",
    "        if goal_func(observed_state,goal):\n",
    "            return 0\n",
    "        \n",
    "        return -1\n",
    "\n",
    "    def push(self, transition):\n",
    "                \n",
    "        if len(self.memory) == self.capacity:\n",
    "            self.memory = self.memory[1:]   \n",
    "        \n",
    "        self.memory.append(transition)\n",
    "        self.current_episode.append(transition)\n",
    "        #goals = \n",
    "            \n",
    "    def add_future_goals(self):\n",
    "        #reward = compute_reward(transition[0],goal)\n",
    "        for i in range(len(self.current_episode)):\n",
    "            current_experience = self.current_episode[i]\n",
    "            observed_state = current_experience[3]\n",
    "    \n",
    "            sampled_trans = random.sample(self.current_episode[i+1:],self.k)\n",
    "  \n",
    "            new_rewards = [self.compute_reward(observed_state,trans[3]) for trans in sampled_trans]\n",
    "            for new_r in new_rewards:\n",
    "                current_experience[2] = new_r\n",
    "                self.memory.append(current_experience)\n",
    "                \n",
    "        self.current_episode = []\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.memory,batch_size)\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "def train_hindsight(model, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float, device = model.device)\n",
    "    action = torch.tensor(action, dtype=torch.int64, device = model.device)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float, device = model.device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float, device = model.device)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_val(model, state, action):\n",
    "    # YOUR CODE HERE\n",
    "    Q_approx = model(state)\n",
    "    action_values = torch.gather(Q_approx, dim=1, index=action.reshape(-1,1))\n",
    "    \n",
    "    return action_values\n",
    "    \n",
    "def compute_target(model, reward, next_state, done, discount_factor):\n",
    "    # done is a boolean (vector) that indicates if next_state is terminal (episode is done)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    Q_approx = model(next_state)\n",
    "    max_Q = torch.max(Q_approx,dim=1)[0]\n",
    "    target = reward + discount_factor * max_Q\n",
    "\n",
    "    indices = torch.tensor(np.where(done),dtype=torch.long, device = model.device)\n",
    "    target = target.scatter(0, indices.reshape(-1), 0)\n",
    "    target = target.reshape(-1,1)\n",
    "        \n",
    "    return target\n",
    "\n",
    "def train(model, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float, device = model.device)\n",
    "    action = torch.tensor(action, dtype=torch.int64, device = model.device)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float, device = model.device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float, device = model.device)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(train,select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate, \n",
    "                 beta_max_iter = 1000,beta_start = 0.4,beta_end = 1.0):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    \n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        # YOUR CODE HERE\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        local_steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            epsilon = get_epsilon(global_steps)\n",
    "            #if memory.name == \" hindsight\":\n",
    "            #    goal = ..\n",
    "            #    a = select_action_func(model, s, epsilon,goal)\n",
    "            #else:\n",
    "            a = select_action_func(model, s, epsilon)\n",
    "            observation,reward,done,info = env.step(a)\n",
    "            \n",
    "            global_steps += 1\n",
    "            local_steps += 1\n",
    "            \n",
    "            if memory.name == \"prioritized\":\n",
    "                memory.add(s, a, reward, observation, float(done))\n",
    "                beta = parameter_schedule(global_steps,beta_max_iter,beta_start,beta_end)\n",
    "                loss = train(model, memory, optimizer, batch_size, discount_factor,beta)\n",
    "            #elif memory.name == \"hindsight\":\n",
    "            #    memory.push((s, a, reward, observation, done),goal) \n",
    "            #    loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "            else: #memory.name == \"uniform\" or memory.name == \"selective\" or memory.name == \"hindsight\":\n",
    "                memory.push((s, a, reward, observation, done))\n",
    "                loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "            s = observation\n",
    "        \n",
    "        episode_durations.append(local_steps)\n",
    "        \n",
    "        if memory.name == \"hindsight\":\n",
    "            memory.add_future_goals()\n",
    "        \n",
    "        #Check for convergance to terminate perhaps?\n",
    "\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set multiple seeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will seed the algorithm (before initializing QNetwork!) for reproducability\n",
    "\n",
    "if \"cuda\" in device:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42 \n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "np.random.seed(seed) #Added this seed for numpy, as used in selection action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = env_cartpole\n",
    "env = env_car\n",
    "#env = env_pen\n",
    "#env = env_acrobot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_reward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-fe9bcb4469b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m episode_durations = run_episodes(train_func,select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate,\n\u001b[1;32m---> 60\u001b[1;33m                                 beta_max_iter,beta_start,beta_end)\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-33f0c81c7aaf>\u001b[0m in \u001b[0;36mrun_episodes\u001b[1;34m(train, select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate, beta_max_iter, beta_start, beta_end)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"hindsight\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_future_goals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m#Check for convergance to terminate perhaps?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-009cf03b5080>\u001b[0m in \u001b[0;36madd_future_goals\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0msampled_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mnew_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobserved_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msampled_trans\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mnew_r\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_rewards\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0mcurrent_experience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_r\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-009cf03b5080>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0msampled_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_episode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mnew_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobserved_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msampled_trans\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mnew_r\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_rewards\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0mcurrent_experience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_r\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_reward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "buffer_size = 10000\n",
    "\n",
    "#Parameters for network, e.g. hidden dim\n",
    "num_input = len(env.observation_space.sample())\n",
    "num_hidden = 128\n",
    "num_output = env.action_space.n\n",
    "\n",
    "#Parameters for schedule of beta and alpha for prioritized replay\n",
    "beta_max_iter = 1000\n",
    "beta_start = 0.4\n",
    "beta_end = 1.0\n",
    "prioritized_replay_alpha = 0.6\n",
    "#prioritized_replay_eps=1e-6 Set default in the training function\n",
    "\n",
    "#Parameters for selective\n",
    "episodic_buffer_size = 12000\n",
    "\n",
    "#Parameters for hindsight\n",
    "num_policy_hidden = 128\n",
    "k = 4\n",
    "env_name = env.unwrapped.spec.id\n",
    "bigger_buffer_size = 10000 * k\n",
    "\n",
    "#Picking the memory\n",
    "memory_method = 4\n",
    "if memory_method == 1:  #Uniform\n",
    "    memory = ReplayMemory(buffer_size)\n",
    "    train_func = train #Uniform\n",
    "    select_action_func = select_action #epsilon greedy\n",
    "    \n",
    "elif memory_method == 2:   #Prioritzed\n",
    "    memory = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "    train_func = train_prioritized #Prioritized Replay\n",
    "    select_action_func = select_action #epsilon greedy\n",
    "\n",
    "elif memory_method == 3:   #Selective\n",
    "    memory = SelectiveReplayMemory(buffer_size,episodic_buffer_size)\n",
    "    train_func = train #same way as uniform\n",
    "    select_action_func = select_action #epsilon greedy\n",
    "\n",
    "elif memory_method == 4: #Hindsight\n",
    "    memory = HindsightReplayMemory(bigger_buffer_size,k,env_name)\n",
    "    train_func = train #same way as uniform\n",
    "    #select_action_func = Epsilon_Greedy_Goal(device,2*num_input,num_policy_hidden,num_output)\n",
    "    select_action_func = select_action #epsilon greedy\n",
    "    \n",
    "\n",
    "#Initializing model\n",
    "model = QNetwork(device,num_input,num_hidden,num_output)\n",
    "model = model.to(device)\n",
    "\n",
    "#Running\n",
    "start_time = time.time()\n",
    "episode_durations = run_episodes(train_func,select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate,\n",
    "                                beta_max_iter,beta_start,beta_end)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total duration time: \", str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With simulation visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
