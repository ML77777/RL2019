{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import pickle\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marvi\\Anaconda3\\envs\\rl2019\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n",
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env_cartpole = gym.envs.make(\"CartPole-v0\") # Has two actions, See doc for mor info, ??env_cartpole.env\n",
    "env_car = gym.envs.make(\"MountainCar-v0\")   # Has three actions\n",
    "env_pen = gym.envs.make(\"Pendulum-v0\")      # Has continious action values like array([-1.2552279] or array([1.7774895] \n",
    "env_acrobot = gym.envs.make(\"Acrobot-v1\")   # Has Three actions, applying +1, 0 or -1 torque on the joint between #\n",
    "                                            # the two pendulum links.   See doc for more info, ??env_acrobot.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??env_acrobot.env\n",
    "#env_acrobot.action_space.sample()\n",
    "env = env_cartpole\n",
    "#env = env_car\n",
    "#env = env_acrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  \n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test demo environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "    #print(obs)\n",
    "env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, device,num_input=4,num_hidden=128,num_output=2):\n",
    "        nn.Module.__init__(self)\n",
    "        self.device = device\n",
    "        self.l1 = nn.Linear(num_input, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, num_output)\n",
    "        self.input_dim = num_input\n",
    "        self.hidden_dim = num_hidden\n",
    "        self.output_dim = num_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.Tensor(x)\n",
    "            \n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        forward_pass = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.ReLU(),\n",
    "            self.l2\n",
    "        )\n",
    "        \n",
    "        return forward_pass(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon ($\\epsilon$) greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(it):\n",
    "    # YOUR CODE HERE\n",
    "    epsilon = 1 - (min(it,1000) * 0.00095) #After 1000 iterations epsilon should be 0.05\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_schedule(it,max_iter,initial_value,final_value):\n",
    "    #Following general formula of get epislon\n",
    "    parameter_value = initial_value - (min(it,max_iter) * ( (initial_value - final_value) / max_iter) )\n",
    "                            \n",
    "    return parameter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon):\n",
    "    # YOUR CODE HERE\n",
    "    with torch.no_grad():\n",
    "        Q_approx = model(state)\n",
    "        a = int(np.random.rand() * model.output_dim) if np.random.rand() < epsilon else torch.argmax(Q_approx).item()\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replays types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.name = \"uniform\"\n",
    "\n",
    "    def push(self, transition):\n",
    "        \n",
    "        if len(self.memory) == self.capacity:\n",
    "            self.memory = self.memory[1:]   \n",
    "        \n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prioritized Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_tree import SumSegmentTree, MinSegmentTree\n",
    "\n",
    "#Source OpenAI: https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, size, alpha):\n",
    "        \"\"\"Create Prioritized Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        alpha: float\n",
    "            how much prioritization is used\n",
    "            (0 - no prioritization, 1 - full prioritization)\n",
    "        See Also\n",
    "        --------\n",
    "        ReplayBuffer.__init__\n",
    "        \"\"\"\n",
    "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
    "        assert alpha >= 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "        self.name = \"prioritized\"\n",
    "\n",
    "    def add(self, *args, **kwargs):\n",
    "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
    "        idx = self._next_idx\n",
    "        super().add(*args, **kwargs)\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        p_total = self._it_sum.sum(0, len(self._storage) - 1)\n",
    "        every_range_len = p_total / batch_size\n",
    "        for i in range(batch_size):\n",
    "            mass = random.random() * every_range_len + i * every_range_len\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        compared to ReplayBuffer.sample\n",
    "        it also returns importance weights and idxes\n",
    "        of sampled experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - no corrections, 1 - full correction)\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        weights: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "            denoting importance weight of each sampled transition\n",
    "        idxes: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "            idexes in buffer of sampled experiences\n",
    "        \"\"\"\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = np.array(weights)\n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return tuple(list(encoded_sample) + [weights, idxes])\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\n",
    "        sets priority of transition at index idxes[i] in buffer\n",
    "        to priorities[i].\n",
    "        Parameters\n",
    "        ----------\n",
    "        idxes: [int]\n",
    "            List of idxes of sampled transitions\n",
    "        priorities: [float]\n",
    "            List of updated priorities corresponding to\n",
    "            transitions at the sampled idxes denoted by\n",
    "            variable `idxes`.\n",
    "        \"\"\"\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = priority ** self._alpha\n",
    "            self._it_min[idx] = priority ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)\n",
    "            \n",
    "def train_prioritized(model, memory, optimizer, batch_size, discount_factor,beta,prioritized_replay_eps=1e-6):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    #----------------------------#Adjusted of normal train--------------------------------------------------------\n",
    "    transitions = memory.sample(batch_size, beta=beta)\n",
    "    (state, action, reward, next_state, done, weights, batch_idxes) = transitions#zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float, device = model.device)\n",
    "    action = torch.tensor(action, dtype=torch.int64, device = model.device)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float, device = model.device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float, device = model.device)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    weights = torch.tensor(weights, dtype=torch.float, device = model.device)\n",
    "    #batch_idxes = torch.tensor(batch_idxes, dtype = torch.int64, device = model.device)\n",
    "    \n",
    "     #-------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    #loss = F.smooth_l1_loss(q_val, target)\n",
    "    td_errors = F.smooth_l1_loss(q_val, target,reduction=\"none\") #TD errors, \n",
    "\n",
    "    weighted_td_errors = weights.reshape(-1,1) * td_errors \n",
    "    #weighted_td_errors = td_errors\n",
    "    \n",
    "    loss = torch.mean(weighted_td_errors) \n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    mean_loss_value = loss.item() \n",
    "    \n",
    "    if model.device == \"cpu\":\n",
    "        #td_errors = td_errors.detach().numpy()\n",
    "        weighted_td_errors = weighted_td_errors.detach().numpy()\n",
    "    else:\n",
    "        weighted_td_errors = weighted_td_errors.cpu().detach().numpy()\n",
    "    new_priorities = np.abs(weighted_td_errors) + prioritized_replay_eps\n",
    "    #new_priorities = np.abs(loss_value) + prioritized_replay_eps\n",
    "    memory.update_priorities(batch_idxes, new_priorities)\n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return mean_loss_value  # Returns a Python scalar, and releases history (similar to .detach())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selective Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveReplayMemory:\n",
    "    \n",
    "    def __init__(self, fifo_capacity, episodic_memory_capacity):\n",
    "        self.fifo_capacity = fifo_capacity\n",
    "        self.episodic_capacity = episodic_memory_capacity\n",
    "        self.fifo_memory = []\n",
    "        #self.episodic_memory = []\n",
    "        self.episodic_memory = [ 1 for i in range(episodic_memory_capacity)]\n",
    "        self.rank_values = np.ones(episodic_memory_capacity+1)\n",
    "        self.name = \"selective\"\n",
    "        self.episodic_len = 0\n",
    "\n",
    "    def push(self, transition):\n",
    "        \n",
    "        if len(self.fifo_memory) == self.fifo_capacity:\n",
    "            self.fifo_memory = self.fifo_memory[1:] \n",
    "            #self.fifo_len -= 1\n",
    "        \n",
    "        self.fifo_memory.append(transition)\n",
    "    \n",
    "        #Distribution matching selection\n",
    "        rank_value = np.random.standard_normal(1)\n",
    "        \n",
    "        if self.episodic_len == self.episodic_capacity:\n",
    "            self.rank_values[-1] = rank_value\n",
    "\n",
    "            lowest_rank_value_idx = np.argmin(self.rank_values)\n",
    "            if lowest_rank_value_idx < self.episodic_capacity:\n",
    "                self.rank_values[lowest_rank_value_idx] = rank_value\n",
    "                self.episodic_memory[lowest_rank_value_idx] = transition\n",
    "\n",
    "        else:\n",
    "            idx = self.episodic_len\n",
    "            self.rank_values[idx] = rank_value\n",
    "            self.episodic_memory[idx] = transition\n",
    "            self.episodic_len += 1\n",
    "            #self.episodic_memory.append( transition )\n",
    "            #self.episodic_len += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        #concatenate and sample\n",
    "        concat = self.fifo_memory + self.episodic_memory[:self.episodic_len]\n",
    "        samples = random.sample(concat,batch_size)\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fifo_memory) #+ len(episodic_memory) #Do we count both memories as len? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hindsight Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HindsightReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity, k,env_name):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.k = k\n",
    "        self.name = \"hindsight\"\n",
    "        self.goals = []\n",
    "        self.env_name = env_name\n",
    "        self.current_episode = []\n",
    "            \n",
    "        if env_name == \"Acrobot-v1\":\n",
    "            self.true_goal_func = lambda s: bool(-np.cos(s[0]) - np.cos(s[1] + s[0]) > 1.)  #This is probably wrong from the doc?\n",
    "            self.true_goal_example = np.array([-0.27671839,  0.96095106,  0.25269736,  0.96754537,  1.35394005, -0.99966234])\n",
    "            self.goal_func = lambda s1,s2: (s1 == s2).all()\n",
    "        elif env_name == \"MountainCar-v0\":\n",
    "            self.true_goal_func = lambda s: s[0] >= 0.5 \n",
    "            self.true_goal_example = np.array([0.5, 0.00336499])\n",
    "            self.goal_func = lambda s1,s2: s1[0] == s2[0]\n",
    "        \n",
    "    def compute_reward(self,observed_state,goal):\n",
    "        \n",
    "        if self.goal_func(observed_state,goal):\n",
    "            return 0\n",
    "        \n",
    "        return -1\n",
    "\n",
    "    def push(self, transition):\n",
    "                \n",
    "        if len(self.memory) == self.capacity:\n",
    "            self.memory = self.memory[1:]   \n",
    "        \n",
    "        self.memory.append(transition)\n",
    "        self.current_episode.append(transition)\n",
    "        #goals = \n",
    "            \n",
    "    def add_future_goals(self):\n",
    "\n",
    "        episode_len = len(self.current_episode)\n",
    "        state_size = len(self.true_goal_example)\n",
    "        sample_size = self.k \n",
    "        \n",
    "        for i in range(episode_len):\n",
    "            current_transition = self.current_episode[i]\n",
    "            observed_state_goal = current_transition[3]\n",
    "            observed_state = observed_state_goal[:state_size]\n",
    "            \n",
    "            amount_future = (episode_len - (i+1))\n",
    "            if amount_future > 0:\n",
    "                sampled_trans = random.choices(self.current_episode[i+1:],k = sample_size) #Sample with replacement \n",
    "                new_rewards = [self.compute_reward(observed_state,trans[0][:state_size]) for trans in sampled_trans]\n",
    "            \n",
    "                #print(new_rewards)\n",
    "                #if 0 in new_rewards:\n",
    "                #    print(\"-\" *50)\n",
    "                #    print(new_rewards)\n",
    "            \n",
    "                for new_r in new_rewards:\n",
    "                    tmp_trans = list(current_transition)\n",
    "                    tmp_trans[2] = new_r\n",
    "                    tmp_trans = tuple(tmp_trans)\n",
    "                    self.memory.append(tmp_trans)\n",
    "                    #if 0 in new_rewards:\n",
    "                    #    print(current_transition)\n",
    "                    #    print(new_r)\n",
    "                    #    print(tmp_trans)\n",
    "            \n",
    "        self.current_episode = []\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.memory,batch_size)\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "def train_hindsight(model, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float, device = model.device)\n",
    "    action = torch.tensor(action, dtype=torch.int64, device = model.device)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float, device = model.device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float, device = model.device)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_val(model, state, action):\n",
    "    # YOUR CODE HERE\n",
    "    Q_approx = model(state)\n",
    "    action_values = torch.gather(Q_approx, dim=1, index=action.reshape(-1,1))\n",
    "    \n",
    "    return action_values\n",
    "    \n",
    "def compute_target(model, reward, next_state, done, discount_factor):\n",
    "    # done is a boolean (vector) that indicates if next_state is terminal (episode is done)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    Q_approx = model(next_state)\n",
    "    max_Q = torch.max(Q_approx,dim=1)[0]\n",
    "    target = reward + discount_factor * max_Q\n",
    "\n",
    "    indices = torch.tensor(np.where(done),dtype=torch.long, device = model.device)\n",
    "    target = target.scatter(0, indices.reshape(-1), 0)\n",
    "    target = target.reshape(-1,1)\n",
    "        \n",
    "    return target\n",
    "\n",
    "def train(model, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float, device = model.device)\n",
    "    action = torch.tensor(action, dtype=torch.int64, device = model.device)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float, device = model.device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float, device = model.device)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(train,select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate, \n",
    "                 beta_max_iter = 1000,beta_start = 0.4,beta_end = 1.0):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    \n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        # YOUR CODE HERE\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        local_steps = 0 \n",
    "        \n",
    "        while not done:\n",
    "            epsilon = get_epsilon(global_steps)\n",
    "            if memory.name == \"hindsight\":\n",
    "                combined_s = np.concatenate( (s, memory.true_goal_example))\n",
    "                a = select_action_func(model, combined_s, epsilon)\n",
    "            else:\n",
    "                a = select_action_func(model, s, epsilon)\n",
    "            \n",
    "            observation,reward,done,info = env.step(a)\n",
    "            \n",
    "            global_steps += 1\n",
    "            local_steps += 1\n",
    "            \n",
    "            if memory.name == \"prioritized\":\n",
    "                memory.add(s, a, reward, observation, float(done))\n",
    "                beta = parameter_schedule(global_steps,beta_max_iter,beta_start,beta_end)\n",
    "                loss = train(model, memory, optimizer, batch_size, discount_factor,beta)\n",
    "            elif memory.name == \"hindsight\":\n",
    "                combined_observation = np.concatenate( (observation, memory.true_goal_example) )\n",
    "                memory.push((combined_s, a, reward, combined_observation, done)) \n",
    "                loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "            elif memory.name == \"uniform\" or memory.name == \"selective\":\n",
    "                memory.push((s, a, reward, observation, done))\n",
    "                loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "            s = observation\n",
    "        \n",
    "        episode_durations.append(local_steps)\n",
    "        \n",
    "        if memory.name == \"hindsight\":\n",
    "            memory.add_future_goals()\n",
    "        \n",
    "        #Check for convergance to terminate perhaps?\n",
    "\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility save functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,name=\"model\"):\n",
    "    torch.save({\n",
    "        # You can add more here if you need, e.g. critic\n",
    "        'model': model.state_dict()  # Always save weights rather than objects\n",
    "    },\n",
    "    name + \".pt\")\n",
    "    \n",
    "def pickle_action(obj,name,action=\"w\"):\n",
    "    if action == 'r':\n",
    "        return pickle.load( open( name, \"rb\" ) )          \n",
    "    else:\n",
    "        pickle.dump( obj, open( name, \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(env,seeds_idx=[0,1,2,3,4],method_list =[1,2,3,4]):\n",
    "    \n",
    "    #Fixed Hyperparameters\n",
    "    num_episodes = 1500\n",
    "    batch_size = 64\n",
    "    discount_factor = 0.8\n",
    "    learn_rate = 1e-3\n",
    "    buffer_size = 10000\n",
    "    \n",
    "    env_name = env.unwrapped.spec.id\n",
    "    save_name = env_name + \"_final_results.p\"\n",
    "    save_name_intermediate = env_name + \"_intermediate_results.p\"\n",
    "    \n",
    "    if env_name == \"MountainCar-v0\":\n",
    "        env._max_episode_steps = 400\n",
    "        \n",
    "    #Parameters for schedule of beta and alpha for prioritized replay\n",
    "    beta_max_iter = env._max_episode_steps * 0.5 * num_episodes\n",
    "    beta_start = 0.4\n",
    "    beta_end = 0.8\n",
    "    prioritized_replay_alpha = 0.4\n",
    "\n",
    "    #prioritized_replay_eps=1e-6 Set default in the training function\n",
    "    episodic_buffer_size = buffer_size * 6\n",
    "\n",
    "    #Parameters for hindsight\n",
    "    k = 4\n",
    "    bigger_buffer_size = buffer_size * 2*k\n",
    "\n",
    "    all_durations = []\n",
    "    all_info = []\n",
    "\n",
    "    #Parameters for network, e.g. hidden dim\n",
    "    original_num_input = len(env.observation_space.sample())\n",
    "    num_hidden = 128\n",
    "    num_output = env.action_space.n\n",
    "    \n",
    "    seeds = [17,50,60,21,5]\n",
    "    using_seeds = []\n",
    "    \n",
    "    for seed_idx in seeds_idx:\n",
    "        using_seeds.append(seeds[seed_idx])\n",
    "\n",
    "    for seed in using_seeds:\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        for memory_method in method_list:\n",
    "            #print((memory_method == 4) and (env_name == \"CartPole-v0\"))\n",
    "            if (memory_method == 4) and (env_name == \"CartPole-v0\"):\n",
    "                continue\n",
    "                 \n",
    "            if memory_method == 1:  #Uniform\n",
    "                memory = ReplayMemory(buffer_size)\n",
    "                num_input = original_num_input\n",
    "                train_func = train #Uniform\n",
    "                select_action_func = select_action #epsilon greedy\n",
    "                #save_name = \"ER_\" + env_name + \"_results.p\"\n",
    "\n",
    "            elif memory_method == 2:   #Prioritzed\n",
    "                memory = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "                num_input = original_num_input\n",
    "                train_func = train_prioritized #Prioritized Replay\n",
    "                select_action_func = select_action #epsilon greedy\n",
    "                #save_name = \"PER_\" + env_name + \"_results.p\"\n",
    "\n",
    "            elif memory_method == 3:   #Selective\n",
    "                memory = SelectiveReplayMemory(buffer_size,episodic_buffer_size)\n",
    "                num_input = original_num_input\n",
    "                train_func = train #same way as uniform\n",
    "                select_action_func = select_action #epsilon greedy\n",
    "                #save_name = \"SER_\" + env_name + \"_results.p\"\n",
    "\n",
    "            elif memory_method == 4: #Hindsight\n",
    "                num_input = 2 * original_num_input\n",
    "                memory = HindsightReplayMemory(bigger_buffer_size,k,env_name)\n",
    "                train_func = train #same way as uniform\n",
    "                select_action_func = select_action #epsilon greedy\n",
    "                #save_name = \"HER_\" + env_name + \"_results.p\"\n",
    "\n",
    "            #Initialize model\n",
    "            model = QNetwork(device,num_input,num_hidden,num_output)\n",
    "            model = model.to(device)\n",
    "\n",
    "            #Running\n",
    "            start_time = time.time()\n",
    "            episode_durations = run_episodes(train_func,select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate,\n",
    "                                                beta_max_iter,beta_start,beta_end)\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(\"Total duration time: \", str(end_time - start_time))\n",
    "\n",
    "            all_durations.append(episode_durations)\n",
    "\n",
    "                #all_info.append( (env_name,\"num_episodes: \" + str(num_episodes), \"seed: \" + str(seed), \n",
    "                #                \"beta_start: \" + str(beta_start),\"beta_end: \" + str(beta_end),\"beta max iter: \" + str(beta_max_iter)) )\n",
    "\n",
    "                #fig = plt.figure(figsize=(10,3))\n",
    "                #plt.plot(smooth(episode_durations, 1))\n",
    "                #plt.title('Episode durations per episode for ' + env_name + \" with \" + memory.name)\n",
    "                #plt.show()\n",
    "    \n",
    "            pickle_action(all_durations, save_name_intermediate, action=\"w\")\n",
    "        \n",
    "    pickle_action(all_durations, save_name, action=\"w\")\n",
    "    \n",
    "    return all_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-2515f687f4a1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-2515f687f4a1>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Manually run env experiment\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Manually run env experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_durations_cartpole = run_experiment(env_cartpole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_durations_car = run_experiment(env_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_durations_acrobot = run_experiment(env_acrobot,[3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run only method 3, selective\n",
    "all_durations_cartpole = run_experiment(env_cartpole,method_list=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run only method 3, selective\n",
    "all_durations_car = run_experiment(env_car,method_list=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run only method 3, selective\n",
    "all_durations_acrobot = run_experiment(env_acrobot,method_list=[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode for ' + env_name + \" with \" + memory.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set multiple seeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will seed the algorithm (before initializing QNetwork!) for reproducability\n",
    "\n",
    "if \"cuda\" in device:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42 \n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "np.random.seed(seed) #Added this seed for numpy, as used in selection action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = env_cartpole\n",
    "#env = env_car\n",
    "#env = env_pen\n",
    "env = env_acrobot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env._max_episode_steps = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "buffer_size = 10000\n",
    "\n",
    "#Parameters for network, e.g. hidden dim\n",
    "num_input = len(env.observation_space.sample())\n",
    "num_hidden = 128\n",
    "num_output = env.action_space.n\n",
    "\n",
    "#Parameters for schedule of beta and alpha for prioritized replay\n",
    "beta_max_iter = 1000\n",
    "beta_start = 0.4\n",
    "beta_end = 1.0\n",
    "prioritized_replay_alpha = 0.6\n",
    "#prioritized_replay_eps=1e-6 Set default in the training function\n",
    "\n",
    "#Parameters for selective\n",
    "episodic_buffer_size = 12000\n",
    "\n",
    "#Parameters for hindsight\n",
    "k = 4\n",
    "env_name = env.unwrapped.spec.id\n",
    "bigger_buffer_size = 10000 * k\n",
    "\n",
    "#Picking the memory type\n",
    "memory_method = 3\n",
    "\n",
    "if memory_method == 1:  #Uniform\n",
    "    memory = ReplayMemory(buffer_size)\n",
    "    train_func = train #Uniform\n",
    "    select_action_func = select_action #epsilon greedy\n",
    "    \n",
    "elif memory_method == 2:   #Prioritzed\n",
    "    memory = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "    train_func = train_prioritized #Prioritized Replay\n",
    "    select_action_func = select_action #epsilon greedy\n",
    "\n",
    "elif memory_method == 3:   #Selective\n",
    "    memory = SelectiveReplayMemory(buffer_size,episodic_buffer_size)\n",
    "    train_func = train #same way as uniform\n",
    "    select_action_func = select_action #epsilon greedy\n",
    "\n",
    "elif memory_method == 4: #Hindsight\n",
    "    num_input = 2 * num_input\n",
    "    memory = HindsightReplayMemory(bigger_buffer_size,k,env_name)\n",
    "    train_func = train #same way as uniform\n",
    "    select_action_func = select_action #epsilon greedy\n",
    "\n",
    "#Initialize model\n",
    "model = QNetwork(device,num_input,num_hidden,num_output)\n",
    "model = model.to(device)\n",
    "\n",
    "#Running\n",
    "start_time = time.time()\n",
    "episode_durations = run_episodes(train_func,select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate,\n",
    "                                beta_max_iter,beta_start,beta_end)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total duration time: \", str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_acrobot]: #[env_cartpole, env_car, env_acrobot]:\n",
    "#268.72109293937683\n",
    "#467.59742856025\n",
    "#283.3788323402405\n",
    "#174.60441517829895\n",
    "#72.91515350341797\n",
    "#69.80593180656433\n",
    "\n",
    "#plt.plot(smooth(episode_durations, 10))\n",
    "#plt.title('Episode durations per episode for ' + env_name + \" with \" + memory.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prioritized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed Hyperparameters\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "buffer_size = 10000\n",
    "\n",
    "#Parameters for selective\n",
    "episodic_buffer_size = buffer_size * 6\n",
    "\n",
    "#Parameters for hindsight\n",
    "k = 6\n",
    "env_name = env.unwrapped.spec.id\n",
    "bigger_buffer_size = 10000 * k\n",
    "\n",
    "all_durations = []\n",
    "all_info = []\n",
    "memory_method = 2\n",
    "beta_max_iter = env._max_episode_steps * 0.5 * num_episodes\n",
    "\n",
    "counter = 0 \n",
    "for env in [env_acrobot]: #[env_cartpole, env_car, env_acrobot]:\n",
    "    env_name = env.unwrapped.spec.id\n",
    "    #if env_name == \"MountainCar-v0\":\n",
    "    #    num_episodes = 1500\n",
    "    #else:\n",
    "    #    num_episodes = 1000\n",
    "    \n",
    "    #Parameters for network, e.g. hidden dim\n",
    "    num_input = len(env.observation_space.sample())\n",
    "    num_hidden = 128\n",
    "    num_output = env.action_space.n\n",
    "\n",
    "    for seed in [7,25]:#,17,50,60]:\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed) \n",
    "    \n",
    "        for beta_start in [0,0.2,0.4]:\n",
    "            for beta_end in [0.6,0.8,1.0]:\n",
    "                for prioritized_replay_alpha in [0.4,0.6,0.8]:\n",
    "\n",
    "                    if memory_method == 1:  #Uniform\n",
    "                        memory = ReplayMemory(buffer_size)\n",
    "                        train_func = train #Uniform\n",
    "                        select_action_func = select_action #epsilon greedy\n",
    "\n",
    "                    elif memory_method == 2:   #Prioritzed\n",
    "                        memory = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "                        train_func = train_prioritized #Prioritized Replay\n",
    "                        select_action_func = select_action #epsilon greedy\n",
    "\n",
    "                    elif memory_method == 3:   #Selective\n",
    "                        memory = SelectiveReplayMemory(buffer_size,episodic_buffer_size)\n",
    "                        train_func = train #same way as uniform\n",
    "                        select_action_func = select_action #epsilon greedy\n",
    "\n",
    "                    elif memory_method == 4: #Hindsight\n",
    "                        num_input = 2 * num_input\n",
    "                        memory = HindsightReplayMemory(bigger_buffer_size,k,env_name)\n",
    "                        train_func = train #same way as uniform\n",
    "                        select_action_func = select_action #epsilon greedy\n",
    "\n",
    "                    #Initialize model\n",
    "                    model = QNetwork(device,num_input,num_hidden,num_output)\n",
    "                    model = model.to(device)\n",
    "\n",
    "                    #Running\n",
    "                    #start_time = time.time()\n",
    "                    #episode_durations = run_episodes(train_func,select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate,\n",
    "                    #                                beta_max_iter,beta_start,beta_end)\n",
    "                    #end_time = time.time()\n",
    "\n",
    "                    #print(\"Total duration time: \", str(end_time - start_time))\n",
    "                    \n",
    "                    #all_durations.append(episode_durations)\n",
    "\n",
    "                    #all_info.append( (env_name,\"num_episodes: \" + str(num_episodes), \"seed: \" + str(seed), \n",
    "                    #                \"beta_start: \" + str(beta_start),\"beta_end: \" + str(beta_end),\"beta max iter: \" + str(beta_max_iter)) )\n",
    "                    if counter == 21:\n",
    "                        print(beta_start)\n",
    "                        print(beta_end)\n",
    "                        print(prioritized_replay_alpha)\n",
    "                    counter += 1\n",
    "                    \n",
    "                    #fig = plt.figure(figsize=(10,3))\n",
    "                    #plt.plot(smooth(episode_durations, 1))\n",
    "                    #plt.title('Episode durations per episode for ' + env_name + \" with \" + memory.name)\n",
    "                    #plt.show()\n",
    "                \n",
    "#pickle_action( \"PER_hyper_results.p\",action=\"w\",[all_durations,all_info])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed Hyperparameters\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "buffer_size = 10000\n",
    "\n",
    "#Parameters for schedule of beta and alpha for prioritized replay\n",
    "beta_max_iter = 1000\n",
    "beta_start = 0.4\n",
    "beta_end = 1.0\n",
    "prioritized_replay_alpha = 0.6\n",
    "#prioritized_replay_eps=1e-6 Set default in the training function\n",
    "\n",
    "#Parameters for selective\n",
    "episodic_buffer_size = 12000\n",
    "\n",
    "#Parameters for hindsight\n",
    "k = 4\n",
    "env_name = env.unwrapped.spec.id\n",
    "bigger_buffer_size = 10000 * k\n",
    "\n",
    "all_durations = []\n",
    "all_info = []\n",
    "memory_method = 3\n",
    "\n",
    "for env in [env_acrobot]: #[env_cartpole, env_car, env_acrobot]:\n",
    "    env_name = env.unwrapped.spec.id\n",
    "    \n",
    "    #Parameters for network, e.g. hidden dim\n",
    "    num_input = len(env.observation_space.sample())\n",
    "    num_hidden = 128\n",
    "    num_output = env.action_space.n\n",
    "\n",
    "    for seed in [7,25]:#,17,50,60]:\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed) \n",
    "        \n",
    "        for episodic_buffer_size in [buffer_size + 5000, buffer_size * 2, buffer_size * 4, buffer_size * 6]:\n",
    "\n",
    "            if memory_method == 1:  #Uniform\n",
    "                memory = ReplayMemory(buffer_size)\n",
    "                train_func = train #Uniform\n",
    "                select_action_func = select_action #epsilon greedy\n",
    "\n",
    "            elif memory_method == 2:   #Prioritzed\n",
    "                memory = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "                train_func = train_prioritized #Prioritized Replay\n",
    "                select_action_func = select_action #epsilon greedy\n",
    "\n",
    "            elif memory_method == 3:   #Selective\n",
    "                memory = SelectiveReplayMemory(buffer_size,episodic_buffer_size)\n",
    "                train_func = train #same way as uniform\n",
    "                select_action_func = select_action #epsilon greedy\n",
    "\n",
    "            elif memory_method == 4: #Hindsight\n",
    "                num_input = 2 * num_input\n",
    "                memory = HindsightReplayMemory(bigger_buffer_size,k,env_name)\n",
    "                train_func = train #same way as uniform\n",
    "                select_action_func = select_action #epsilon greedy\n",
    "\n",
    "            #Initialize model\n",
    "            model = QNetwork(device,num_input,num_hidden,num_output)\n",
    "            model = model.to(device)\n",
    "\n",
    "            #Running\n",
    "            start_time = time.time()\n",
    "            episode_durations = run_episodes(train_func,select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate,\n",
    "                                                    beta_max_iter,beta_start,beta_end)\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(\"Total duration time: \", str(end_time - start_time))\n",
    "                    \n",
    "            all_durations.append(episode_durations)\n",
    "\n",
    "            all_info.append( (env_name,\"num_episodes: \" + str(num_episodes), \"seed: \" + str(seed),\"normal buffer size: \" + str(buffer_size), \n",
    "                            \"episodic buffer size: \" + str(episodic_buffer_size)) ) \n",
    "\n",
    "            #fig = plt.figure(figsize=(10,3))\n",
    "            #plt.plot(smooth(episode_durations, 1))\n",
    "            #plt.title('Episode durations per episode for ' + env_name + \" with \" + memory.name)\n",
    "            #plt.show()\n",
    "                \n",
    "pickle_action( \"SER_hyper_results.p\",action=\"w\",[all_durations,all_info])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hindsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed Hyperparameters\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "buffer_size = 10000\n",
    "\n",
    "#Parameters for schedule of beta and alpha for prioritized replay\n",
    "beta_max_iter = 1000\n",
    "beta_start = 0.4\n",
    "beta_end = 1.0\n",
    "prioritized_replay_alpha = 0.6\n",
    "#prioritized_replay_eps=1e-6 Set default in the training function\n",
    "\n",
    "#Parameters for selective\n",
    "episodic_buffer_size = 12000\n",
    "\n",
    "#Parameters for hindsight\n",
    "k = 4\n",
    "bigger_buffer_size = 10000 * k\n",
    "env_name = env.unwrapped.spec.id\n",
    "\n",
    "all_durations = []\n",
    "all_info = []\n",
    "memory_method = 4\n",
    "\n",
    "for env in [env_acrobot]: #[env_cartpole, env_car, env_acrobot]:\n",
    "    env_name = env.unwrapped.spec.id\n",
    "    \n",
    "    #Parameters for network, e.g. hidden dim\n",
    "    num_input = 2 * len(env.observation_space.sample())\n",
    "    num_hidden = 128\n",
    "    num_output = env.action_space.n\n",
    "    \n",
    "    for seed in [7,25]:#,17,50,60]:\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed) \n",
    "        \n",
    "        for k in [2,4,6]:\n",
    "            for bigger_buffer_size  in [int(buffer_size*(0.5*k)),buffer_size*k, buffer_size*2*k]:\n",
    "\n",
    "                if memory_method == 1:  #Uniform\n",
    "                    memory = ReplayMemory(buffer_size)\n",
    "                    train_func = train #Uniform\n",
    "                    select_action_func = select_action #epsilon greedy\n",
    "\n",
    "                elif memory_method == 2:   #Prioritzed\n",
    "                    memory = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "                    train_func = train_prioritized #Prioritized Replay\n",
    "                    select_action_func = select_action #epsilon greedy\n",
    "\n",
    "                elif memory_method == 3:   #Selective\n",
    "                    memory = SelectiveReplayMemory(buffer_size,episodic_buffer_size)\n",
    "                    train_func = train #same way as uniform\n",
    "                    select_action_func = select_action #epsilon greedy\n",
    "\n",
    "                elif memory_method == 4: #Hindsight\n",
    "                    #num_input = 2 * num_input\n",
    "                    memory = HindsightReplayMemory(bigger_buffer_size,k,env_name)\n",
    "                    train_func = train #same way as uniform\n",
    "                    select_action_func = select_action #epsilon greedy\n",
    "\n",
    "                #Initialize model\n",
    "                model = QNetwork(device,num_input,num_hidden,num_output)\n",
    "                model = model.to(device)\n",
    "\n",
    "                #Running\n",
    "                start_time = time.time()\n",
    "                episode_durations = run_episodes(train_func,select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate,\n",
    "                                                        beta_max_iter,beta_start,beta_end)\n",
    "                end_time = time.time()\n",
    "\n",
    "                print(\"Total duration time: \", str(end_time - start_time))\n",
    "\n",
    "                all_durations.append(episode_durations)\n",
    "\n",
    "                all_info.append( (env_name,\"num_episodes: \" + str(num_episodes), \"seed: \" + str(seed),\n",
    "                                  \"normal buffer size: \" + str(buffer_size),\n",
    "                                \"bigger buffer size: \" + str(bigger_buffer_size), \"k: \" + str(k)) ) \n",
    "\n",
    "                #fig = plt.figure(figsize=(10,3))\n",
    "                #plt.plot(smooth(episode_durations, 1))\n",
    "                #plt.title('Episode durations per episode for ' + env_name + \" with \" + memory.name)\n",
    "                #plt.show()\n",
    "                \n",
    "pickle_action( \"HER_hyper_results.p\",action=\"w\",[all_durations,all_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_durations,per_info = pickle_action( \"PER_hyper_results.p\",action=\"r\")\n",
    "ser_durations,ser_info = pickle_action( \"SER_hyper_results.p\",action=\"r\")\n",
    "her_durations,her_info = pickle_action( \"HER_hyper_results.p\",action=\"r\")\n",
    "\n",
    "def plot_results(durations,seed = 2,num_cols = 3):\n",
    "    amount = len(durations)\n",
    "    seed_cut = int(amount / 2)\n",
    "\n",
    "    num_rows = int(amount/num_cols)\n",
    "    \n",
    "    all_seeds = []\n",
    "    start = 0\n",
    "    offset = int(amount / seed)\n",
    "    end = offset\n",
    "    \n",
    "    for i in range(seed):\n",
    "        all_seeds.append([durations[start:end]])\n",
    "        start += offset\n",
    "        end += offset\n",
    "    \n",
    "    #seed_1 = durations[:seed_cut]\n",
    "    #seed_2 = durations[seed_cut:]\n",
    "\n",
    "    stacked = np.array(all_seeds)\n",
    "    #print(stacked.shape)\n",
    "    mean_results = np.mean(stacked, axis=0)\n",
    "    #print(mean_results.shape)\n",
    "    mean_results = np.squeeze(mean_results)\n",
    "    #print(mean_results.shape) \n",
    "        \n",
    "    std_results = np.std(stacked, axis=0)\n",
    "    #print(std_results.shape) \n",
    "    std_results = np.squeeze(std_results)\n",
    "    #print(std_results.shape) \n",
    "        \n",
    "    fig = plt.figure(figsize=(18,num_rows*3))\n",
    "    bottom = np.min(mean_results)\n",
    "    top = np.max(mean_results)\n",
    "    #fig = plt.figure(figsize=(13,7))\n",
    "    \n",
    "    for i,mean in enumerate(mean_results):\n",
    "        #ax = plt.subplot(num_rows, num_cols, i+1)\n",
    "        plt\n",
    "        #std = std_results[i]\n",
    "        smoothed = smooth(mean, 10)\n",
    "        #smoothed_std = smooth(std, 10)\n",
    "        plt.plot(smoothed,label=str(i))\n",
    "        plt.title(str(i))\n",
    "        plt.ylim((bottom, top))\n",
    "        #plt.fill_between(np.linspace(0,amount,10),mean+smoothed_std,mean-smoothed_std,alpha=0.2)#,label=\"1-$\\sigma$ predictive variance\")\n",
    "        #plt.fill_between(smoothed,smoothed+smoothed_std,smoothed-smoothed_std,alpha=0.2)#,label=\"1-$\\sigma$ predictive variance\")\n",
    "        plt.legend()\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,7))\n",
    "    '''\n",
    "    for i,mean in enumerate(mean_results[9:18]):\n",
    "        #ax = plt.subplot(num_rows, num_cols, i+1)\n",
    "        plt\n",
    "        #std = std_results[i]\n",
    "        smoothed = smooth(mean, 10)\n",
    "        #smoothed_std = smooth(std, 10)\n",
    "        plt.plot(smoothed,label=str(i+9))\n",
    "        plt.title(str(i+9))\n",
    "        plt.ylim((bottom, top))\n",
    "        #plt.fill_between(np.linspace(0,amount,10),mean+smoothed_std,mean-smoothed_std,alpha=0.2)#,label=\"1-$\\sigma$ predictive variance\")\n",
    "        #plt.fill_between(smoothed,smoothed+smoothed_std,smoothed-smoothed_std,alpha=0.2)#,label=\"1-$\\sigma$ predictive variance\")\n",
    "        plt.legend()\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,7))\n",
    "    \n",
    "    for i,mean in enumerate(mean_results[18:]):\n",
    "        #ax = plt.subplot(num_rows, num_cols, i+1)\n",
    "        plt\n",
    "        #std = std_results[i]\n",
    "        smoothed = smooth(mean, 10)\n",
    "        #smoothed_std = smooth(std, 10)\n",
    "        plt.plot(smoothed,label=str(i+18))\n",
    "        plt.title(str(i+18))\n",
    "        plt.ylim((bottom, top))\n",
    "        #plt.fill_between(np.linspace(0,amount,10),mean+smoothed_std,mean-smoothed_std,alpha=0.2)#,label=\"1-$\\sigma$ predictive variance\")\n",
    "        #plt.fill_between(smoothed,smoothed+smoothed_std,smoothed-smoothed_std,alpha=0.2)#,label=\"1-$\\sigma$ predictive variance\")\n",
    "        plt.legend()\n",
    "    '''\n",
    "plot_results(per_durations,seed = 2)\n",
    "#plot_results(ser_durations,seed = 2)\n",
    "#plot_results(her_durations,seed = 2)\n",
    "#print(len(her_durations))\n",
    "#print(len(her_info))\n",
    "#print(her_durations)\n",
    "#print(\"-\" * 50)\n",
    "#print(her_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_info[21]\n",
    "#Alpha = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(per_durations,seed = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(ser_durations,seed = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_info[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(her_durations,seed = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "her_info[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "num_episodes = 2000\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "buffer_size = 10000\n",
    "\n",
    "#Parameters for network, e.g. hidden dim\n",
    "num_input = len(env.observation_space.sample())\n",
    "num_hidden = 128\n",
    "num_output = env.action_space.n\n",
    "\n",
    "#Parameters for schedule of beta and alpha for prioritized replay\n",
    "beta_max_iter = 1000\n",
    "beta_start = 0.4\n",
    "beta_end = 1.0\n",
    "prioritized_replay_alpha = 0.6\n",
    "#prioritized_replay_eps=1e-6 Set default in the training function\n",
    "\n",
    "#Parameters for selective\n",
    "episodic_buffer_size = 12000\n",
    "\n",
    "#Parameters for hindsight\n",
    "num_policy_hidden = 128\n",
    "k = 4\n",
    "env_name = env.unwrapped.spec.id\n",
    "bigger_buffer_size = 10000 * k\n",
    "\n",
    "all_results = []\n",
    "for memory_method in [1,2,3]:\n",
    "    #Picking the memory type\n",
    "    #memory_method = 4\n",
    "\n",
    "    if memory_method == 1:  #Uniform\n",
    "        memory = ReplayMemory(buffer_size)\n",
    "        train_func = train #Uniform\n",
    "        select_action_func = select_action #epsilon greedy\n",
    "\n",
    "    elif memory_method == 2:   #Prioritzed\n",
    "        memory = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "        train_func = train_prioritized #Prioritized Replay\n",
    "        select_action_func = select_action #epsilon greedy\n",
    "\n",
    "    elif memory_method == 3:   #Selective\n",
    "        memory = SelectiveReplayMemory(buffer_size,episodic_buffer_size)\n",
    "        train_func = train #same way as uniform\n",
    "        select_action_func = select_action #epsilon greedy\n",
    "\n",
    "    elif memory_method == 4: #Hindsight\n",
    "        num_input = 2 * num_input\n",
    "        memory = HindsightReplayMemory(bigger_buffer_size,k,env_name)\n",
    "        train_func = train #same way as uniform\n",
    "        select_action_func = select_action #epsilon greedy\n",
    "\n",
    "    #Initialize model\n",
    "    model = QNetwork(device,num_input,num_hidden,num_output)\n",
    "    model = model.to(device)\n",
    "\n",
    "    #Running\n",
    "    start_time = time.time()\n",
    "    episode_durations = run_episodes(train_func,select_action_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate,\n",
    "                                    beta_max_iter,beta_start,beta_end)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Total duration time: \", str(end_time - start_time))\n",
    "    all_results.append(episode_durations)\n",
    "    fig = plt.figure(figsize=(13,7))\n",
    "    plt.plot(smooth(episode_durations, 10))\n",
    "    plt.title('Episode durations per episode for ' + env_name + \" with \" + memory.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "hindsight_car = episode_durations[0]\n",
    "plt.title('Episode durations per episode for ' + env_name + \" with \" + memory.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(smooth(all_results[0], 10))\n",
    "plt.title('Episode durations per episode for ' + env_name + \" with Uniform\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(smooth(all_results[1], 10))\n",
    "plt.title('Episode durations per episode for ' + env_name + \" with prioritized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
