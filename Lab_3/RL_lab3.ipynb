{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marvi\\Anaconda3\\envs\\rl2019\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n",
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env_cartpole = gym.envs.make(\"CartPole-v0\") # Has two actions, See doc for mor info, ??env_cartpole.env\n",
    "env_car = gym.envs.make(\"MountainCar-v0\")   # Has three actions\n",
    "env_pen = gym.envs.make(\"Pendulum-v0\")      # Has continious action values like array([-1.2552279] or array([1.7774895] \n",
    "env_acrobot = gym.envs.make(\"Acrobot-v1\")   # Has Three actions, applying +1, 0 or -1 torque on the joint between #\n",
    "                                            # the two pendulum links.   See doc for more info, ??env_acrobot.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??env_acrobot.env\n",
    "#env_acrobot.action_space.sample()\n",
    "env = env_cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  \n",
    "#device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test demo environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, device,num_input=4,num_hidden=128,num_output=2):\n",
    "        nn.Module.__init__(self)\n",
    "        self.device = device\n",
    "        self.l1 = nn.Linear(num_input, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, num_output)\n",
    "        self.input_dim = num_input\n",
    "        self.hidden_dim = num_hidden\n",
    "        self.output_dim = num_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.Tensor(x)\n",
    "            \n",
    "        x = x.to(self.device)\n",
    "        \n",
    "        forward_pass = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.ReLU(),\n",
    "            self.l2\n",
    "        )\n",
    "        \n",
    "        return forward_pass(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon ($\\epsilon$) greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(it):\n",
    "    # YOUR CODE HERE\n",
    "    epsilon = 1 - (min(it,1000) * 0.00095) #After 1000 iterations epsilon should be 0.05\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_schedule(it,max_iter,initial_value,final_value):\n",
    "    #Following general formula of get epislon\n",
    "    parameter_value = initial_value - (min(it,max_iter) * ( (initial_value - final_value) / max_iter) )\n",
    "                            \n",
    "    return parameter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon):\n",
    "    # YOUR CODE HERE\n",
    "    with torch.no_grad():\n",
    "        Q_approx = model(state)\n",
    "        a = int(np.random.rand() * model.output_dim) if np.random.rand() < epsilon else torch.argmax(Q_approx).item()\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replays types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.name = \"uniform\"\n",
    "\n",
    "    def push(self, transition):\n",
    "        \n",
    "        if len(self.memory) == self.capacity:\n",
    "            self.memory = self.memory[1:]   \n",
    "        \n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prioritized Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_tree import SumSegmentTree, MinSegmentTree\n",
    "\n",
    "#Source OpenAI: https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, size, alpha):\n",
    "        \"\"\"Create Prioritized Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        alpha: float\n",
    "            how much prioritization is used\n",
    "            (0 - no prioritization, 1 - full prioritization)\n",
    "        See Also\n",
    "        --------\n",
    "        ReplayBuffer.__init__\n",
    "        \"\"\"\n",
    "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
    "        assert alpha >= 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "        self.name = \"prioritized\"\n",
    "\n",
    "    def add(self, *args, **kwargs):\n",
    "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
    "        idx = self._next_idx\n",
    "        super().add(*args, **kwargs)\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        p_total = self._it_sum.sum(0, len(self._storage) - 1)\n",
    "        every_range_len = p_total / batch_size\n",
    "        for i in range(batch_size):\n",
    "            mass = random.random() * every_range_len + i * every_range_len\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        compared to ReplayBuffer.sample\n",
    "        it also returns importance weights and idxes\n",
    "        of sampled experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - no corrections, 1 - full correction)\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        weights: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "            denoting importance weight of each sampled transition\n",
    "        idxes: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "            idexes in buffer of sampled experiences\n",
    "        \"\"\"\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = np.array(weights)\n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return tuple(list(encoded_sample) + [weights, idxes])\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\n",
    "        sets priority of transition at index idxes[i] in buffer\n",
    "        to priorities[i].\n",
    "        Parameters\n",
    "        ----------\n",
    "        idxes: [int]\n",
    "            List of idxes of sampled transitions\n",
    "        priorities: [float]\n",
    "            List of updated priorities corresponding to\n",
    "            transitions at the sampled idxes denoted by\n",
    "            variable `idxes`.\n",
    "        \"\"\"\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = priority ** self._alpha\n",
    "            self._it_min[idx] = priority ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)\n",
    "            \n",
    "def train_prioritized(model, memory, optimizer, batch_size, discount_factor,beta,prioritized_replay_eps=1e-6):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    #----------------------------#Adjusted of normal train--------------------------------------------------------\n",
    "    transitions = memory.sample(batch_size, beta=beta)\n",
    "    #(s, a, reward, observation, float(done))\n",
    "    #(s, a, reward, observation, done, weights, batch_idxes) = transitions\n",
    "    #(obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = transitions\n",
    "    #print(len(transitions))\n",
    "    #print(transitions)\n",
    "    # \n",
    "    (state, action, reward, next_state, done, weights, batch_idxes) = transitions#zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float, device = model.device)\n",
    "    action = torch.tensor(action, dtype=torch.int64, device = model.device)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float, device = model.device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float, device = model.device)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    weights = torch.tensor(weights, dtype=torch.float, device = model.device)\n",
    "    #batch_idxes = torch.tensor(batch_idxes, dtype = torch.int64, device = model.device)\n",
    "    \n",
    "     #-------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "    \n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    mean_loss_value = loss.item() \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        td_errors = F.smooth_l1_loss(q_val, target,reduction=\"none\") #TD errors, \n",
    "    #td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
    "        if model.device == \"cpu\":\n",
    "            td_errors = td_errors.detach().numpy()\n",
    "        else:\n",
    "            td_errors = td_errors.cpu().detach().numpy()\n",
    "    new_priorities = np.abs(td_errors) + prioritized_replay_eps\n",
    "    #new_priorities = np.abs(loss_value) + prioritized_replay_eps\n",
    "    memory.update_priorities(batch_idxes, new_priorities)\n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return mean_loss_value  # Returns a Python scalar, and releases history (similar to .detach())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_val(model, state, action):\n",
    "    # YOUR CODE HERE\n",
    "    Q_approx = model(state)\n",
    "    action_values = torch.gather(Q_approx, dim=1, index=action.reshape(-1,1))\n",
    "    \n",
    "    return action_values\n",
    "    \n",
    "def compute_target(model, reward, next_state, done, discount_factor):\n",
    "    # done is a boolean (vector) that indicates if next_state is terminal (episode is done)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    Q_approx = model(next_state)\n",
    "    max_Q = torch.max(Q_approx,dim=1)[0]\n",
    "    target = reward + discount_factor * max_Q\n",
    "\n",
    "    indices = torch.tensor(np.where(done),dtype=torch.long, device = model.device)\n",
    "    target = target.scatter(0, indices.reshape(-1), 0)\n",
    "    target = target.reshape(-1,1)\n",
    "        \n",
    "    return target\n",
    "\n",
    "def train(model, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float, device = model.device)\n",
    "    action = torch.tensor(action, dtype=torch.int64, device = model.device)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float, device = model.device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float, device = model.device)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate, \n",
    "                 beta_max_iter = 1000,beta_start = 0.4,beta_end = 1.0):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        # YOUR CODE HERE\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        local_steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            epsilon = get_epsilon(global_steps)\n",
    "            a = select_action(model, s, epsilon)\n",
    "            observation,reward,done,info = env.step(a)\n",
    "            \n",
    "            global_steps += 1\n",
    "            local_steps += 1\n",
    "\n",
    "            if memory.name == \"uniform\":\n",
    "                memory.push((s, a, reward, observation, done))\n",
    "                loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "            elif memory.name == \"prioritized\":\n",
    "                memory.add(s, a, reward, observation, float(done))\n",
    "                beta = parameter_schedule(global_steps,beta_max_iter,beta_start,beta_end)\n",
    "                loss = train(model, memory, optimizer, batch_size, discount_factor,beta)\n",
    "\n",
    "            s = observation\n",
    "        \n",
    "        episode_durations.append(local_steps)\n",
    "        \n",
    "        #Check for convergance to terminate perhaps?\n",
    "\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set multiple seeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will seed the algorithm (before initializing QNetwork!) for reproducability\n",
    "\n",
    "if \"cuda\" in device:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42 \n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "np.random.seed(seed) #Added this seed for numpy, as used in selection action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = env_cartpole\n",
    "#env = env_car\n",
    "#env = env_pen\n",
    "env = env_acrobot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [09:43<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration time:  583.1293511390686\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "buffer_size = 10000\n",
    "\n",
    "#Parameters for network, e.g. hidden dim\n",
    "num_input = len(env.observation_space.sample())\n",
    "num_hidden = 128\n",
    "num_output = env.action_space.n\n",
    "\n",
    "#Parameters for schedule of beta and alpha for prioritized replay\n",
    "beta_max_iter = 1000\n",
    "beta_start = 0.4\n",
    "beta_end = 1.0\n",
    "prioritized_replay_alpha = 0.6\n",
    "#prioritized_replay_eps=1e-6 Set default in the training function\n",
    "\n",
    "#Picking the memory\n",
    "#memory = ReplayMemory(buffer_size)\n",
    "#train_func = train #Uniform\n",
    "\n",
    "memory = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "train_func = train_prioritized #Prioritized Replay\n",
    "\n",
    "#Initializing model\n",
    "model = QNetwork(device,num_input,num_hidden,num_output)\n",
    "model = model.to(device)\n",
    "\n",
    "#Running\n",
    "start_time = time.time()\n",
    "episode_durations = run_episodes(train_func, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate,\n",
    "                                beta_max_iter,beta_start,beta_end)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total duration time: \", str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Episode durations per episode')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With simulation visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
